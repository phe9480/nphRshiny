% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/simulation.nphDesign.R
\name{simulation.nphDesign}
\alias{simulation.nphDesign}
\title{Trial Data Simulations and Analysis Using Weighted Log-rank Test with non-uniform accrual pattern and lost to follow-up}
\usage{
simulation.nphDesign(
  nSim = 3,
  n = 100,
  r = 1,
  A = 21,
  w = 1.5,
  Lambda = NULL,
  drop0 = 0,
  drop1 = 0,
  dist0 = "exponential",
  lam0 = log(2)/12,
  shape0 = NULL,
  scale0 = NULL,
  p10 = NULL,
  S0 = NULL,
  cuts0 = NULL,
  dist1 = "exponential",
  HR = NULL,
  lam1 = log(2)/12 * 0.7,
  shape1 = NULL,
  scale1 = NULL,
  p11 = NULL,
  S1 = NULL,
  cuts1 = NULL,
  targetEvents = c(30, 60),
  sf = "LDOF",
  param = NULL,
  overall.alpha = 0.025,
  p1 = NULL,
  cum.alpha = NULL,
  logrank = "N",
  fws.options = list(fws5),
  parallel = FALSE,
  n.cores = 8,
  seed = 2022,
  out.z = FALSE
)
}
\arguments{
\item{nSim}{Number of trials}

\item{n}{Total number patients in two arms.}

\item{r}{Randomization ratio r:1, where r refers to the experimental arm, eg, r=2 in 2:1 ratio}

\item{A}{Total accrual period in months}

\item{w}{Weight parameter in cumulative enrollment pattern.
The cumulative enrollment at month t is (t / A)^w, eg, at month 6,
the enrollment is N*(6/24)^2 = N/16 for 24 months planned accrual period.}

\item{Lambda}{Cumulative distribution function (CDF) for enrollment on (0, infinity).
For example, uniform enrollment of 20 patients / month for 24 months has
Lambda = function(t){t/24*as.numeric(t<= 24) + as.numeric(t>24)}. When Lambda is specified, A and w are ignored.}

\item{drop0}{Drop Off rate per month, eg, 1\%, for control arm}

\item{drop1}{Drop Off rate per month, eg, 1\%, for experimental arm}

\item{dist0}{Type of distribution for control arm. The following options are available.
(1) "exponential": lam0 required;
(2) "weibull": shape0 and scale0 required;
(3) "piecewise exponential": lam0 and cuts0 required;
(4) "mixture cure rate of exponential": p10 and lam0 required;
(5) "mixture cure rate of weibull": p10, shape0, and scale0 required
(6) "customized": S0 as a survival function is required.}

\item{lam0}{Hazard rates for control arm for intervals; for exponential distribution,
lam0 = log(2) / median; For piecewise exponential distribution, lam0 is a vector of length 1 greater than length of cuts0.}

\item{shape0}{shape parameter for weibull distribution for control arm. Refer to rweibull() for details.}

\item{scale0}{scale parameter for weibull distribution for control arm. Refer to rweibull() for details.}

\item{p10}{cure rate parameter for mixture cure rate distribution for control arm}

\item{S0}{Survival function for customized distribution for control arm.}

\item{cuts0}{Cut points for piecewise exponential distribution for control arm}

\item{dist1}{Type of distribution for experimental arm. In addition to the options in dist0, "Proportional Hazards" is also available to dist1. When selected, HR is required.}

\item{HR}{When dist1 = "Proportional Hazards", HR is required.}

\item{lam1}{Hazard rates for experimental arm for intervals; for exponential distribution,
lam1 = log(2) / median; For piecewise exponential distribution, lam1 is a vector of length 1 greater than length of cuts1.}

\item{shape1}{shape parameter for weibull distribution for experimental arm. Refer to rweibull() for details.}

\item{scale1}{scale parameter for weibull distribution for experimental arm. Refer to rweibull() for details.}

\item{p11}{cure rate parameter for mixture cure rate distribution for experimental arm}

\item{S1}{Survival function for customized distribution for control arm.}

\item{cuts1}{Cut points for piecewise exponential distribution for experimental arm}

\item{targetEvents}{A vector of target events is used to determine DCOs. For example,
100 target events are used to determine IA DCO; and 150 events are used
to determine the FA DCO. Must be integers.}

\item{sf}{Spending function. LanDeMets O'Brien Fleming: "LDOF", LanDeMets Pocock: "LDPK", "HSD": Hwang-Shih-DeCani spending function with parameter param.}

\item{param}{parameter for Hwang-Shih-DeCani spending function}

\item{overall.alpha}{Allocated overall alpha (one-sided) for group sequential design}

\item{p1}{A fixed p value boundary for IAs (one-sided), which is applicable to Haybittle-Peto alpha spending only.}

\item{cum.alpha}{Cumulative alpha spending by analysis, which is applicable to Bespoke method only. Cum.alpha must have the same length as timing.}

\item{logrank}{Indicator whether log-rank test is requested besides the weighted logrank tests. "Y" or "N". Default "Y".
If "Y", the traditional log-rank test will be used based on survdiff() function.
If "N", the weighted log-rank test with weighting function specified in fws will be used.}

\item{fws.options}{Weighting strategies in the following format as examples. If fws.options is provided,
then the nphDesign object's weighting strategy will be ignored. fws can contain multiple weighting strategies.
For example, fws = list(fws1, fws2) means 2 weighting strategies are evaluated, where
fws1 = list(IA = list(lr), FA=list(lr, fh01)); fws2 = list(IA = list(lr), FA=list(fh01)). Each
weighting strategy is specified as following examples for illustration.
\itemize{
\item (1) Single-time analysis using log-rank test: fws1 = list(FA = list(lr));
\item (2) Two interim analyses and 1 final analysis with logrank at IA1,
max(logrank, fleming-harrington(0,1) at IA2, and
max(logrank, fleming-harrington(0,1), fleming-harrington(1,1)) at final):
fws2 = list(IA1 = list(lr), IA2=list(lr, fh01), FA=list(lr,fh01,fh11)).
\item (3) One IA and one FA: stabilized Fleming-Harrington (0,1) at IA,
and max(logrank, stabilized Fleming-Harrington (0, 1)) at FA.
fw3 = list(IA = list(sfh01), FA=list(lr, sfh01)).
\item General format of weighting strategy specification is: (a) the weight
functions for each analysis must be provided in list() object even there is only
1 weight function for that an analysis. (b) The commonly used functions
are directly available including lr: log-rank test; fh01: Fleming-Harrington (0,1) test;
fh11: Fleming-Harrington(1,1) test; fh55: Fleming-Harrington(0.5, 0.5) test.
stabilized versions at median survival time: sfh01, sfh11, sfh55. Modestly
log-rank test: mlr.
(c) User-defined weight function can also be handled, but the weight function
must be defined as a function of survival rate, i.e., fws = function(s){...},
where s is the survival rate S(t-).
\item Options of weighting strategies for exploration: fws.options=list(fws1, fws2, fws3).
}}

\item{parallel}{True/False indicator. If true, use parallel computing weighted log-rank test for each analysis in strategy m}

\item{n.cores}{This will be used if parallelization is TRUE. Default is 8.}

\item{seed}{seed for generating samples. Default 2022}

\item{out.z}{Output test statistics, TRUE/FALSE}

\item{alpha}{Allocated one-sided alpha levels. sum(alpha) is the total type I error.
If alpha spending function a(t) is used for information time c(t1, ..., tK),
then alpha1 = a(t1), alpha2 = a(t2)-a(t1), ..., alphaK = a(tK)-a(t_{K-1}),
and the total alpha for all analyses is a(tK). When alpha is provided, sf is ignored.}

\item{H0}{"Y" or "N" to indicate whether the simulation is for type I error}
}
\value{
An object with a dataframe for each analysis including the following variables:
\describe{
\item{power}{Power for each analysis}
\item{overall.power}{Overall power of the group sequential design}
\item{wlr.simulations}{Simulation results for each simulated study data.
An array with dimensions (nSim, M, K, 5): nSim simulations, M testing strategies
(fws.options), K analyses, 5 variables below. For example, wlr.simulations\link{,1,2,} shows the 5 variables for simulations testing option 1 at analysis 2.
\itemize{
\item z value
\item p value
\item analysis
\item rejection boundary
\item testing result (1 = positive; 0 = negative)
} z value: wlr.simulations\link{,1,2,1}; p value: wlr.simulations\link{,1,2,2}; analysis: wlr.simulations\link{,1,2,3}; rejection boundary: wlr.simulations\link{,1,2,4}; testing results: wlr.simulations\link{,1,2,5}}
\item{lr.power}{Power for each analysis using log-rank test. Available if logrank ="Y"}
\item{lr.overall.power}{Overall power of the group sequential design using logrank test}
\item{lr.simulations}{Simulation results for each simulated study data using logrank test}
}
}
\description{
Simulate Randomized two-arm trial data with the following characteristics:
(1) randomization time (entry time) is generated according to the specified non-uniform accrual pattern,
i.e. the cumulative recruitment at calendar time t is (t/A)^w with weight w and enrollment complete in A months.
w = 1 means uniform enrollment, which is usually not realistic due to graduate sites activation process. It also allows customized enrollment pattern by specifying the cumulative enrollment distribution with domain (0, infinity).
(2) Survival time follows commonly used distributions and also allow customized survival functions.
(3) Allow different drop off rates for both arms.
(4) Data cutoff dates are determined by specified vector of target events for all analyses.
(5) A dataset is generated for each analysis according to the specified number of target events.
Multiple analyses can be specified according to the vector of targetEvents, eg, targetEvents = c(100, 200, 300)
defines 3 analyses at 100, 200, and 300 events separately.
(6) Weighted log-rank test is then performed for each simulated group sequential dataset.
}
\examples{

#Utility functions
lr = nphRshiny:::lr; fh01 = nphRshiny:::fh01; fh11 = nphRshiny:::fh11
fws1 = list(IA1 = list(lr), FA = list(lr))
fws2 = list(IA1 = list(lr), FA = list(fh01))
fws3 = list(IA1 = list(fh01), FA = list(fh01))
fws4 = list(IA1 = list(lr), FA = list(lr, fh01))
fws5 = list(IA1 = list(lr), FA = list(lr, fh01, fh11))
fws6 = list(IA1 = list(lr, fh01), FA = list(lr, fh01))

#Weighted logrank tests options
fws = list(fws1, fws2, fws3, fws4, fws5, fws6)

#Example (1): Simulate 10 samples from proportional HR with HR = 0.7

#Hazard and survival distributions

#Control Arm
m0 = 10; lambda0 = log(2) / m0
h0 = function(t){lambda0}; 
S0 = function(t){exp(-lambda0 * t)}

#Experimental Arm
HR = 0.7; h1 = function(t){lambda0*HR}; 
S1 = function(t){exp(-lambda0 *HR* t)}

#Enrollment
F.entry = function(t){(t/21)^1.5*as.numeric(t <= 21) + as.numeric(t > 21)}

#Drop-off
drop0 = drop1 = 0.03/12

e = rep(NA, 2); DCO=c(24, 36)
eta0 = -log(1-drop0); G0=function(t){1-exp(-eta0*t)}
eta1 = -log(1-drop1); G1=function(t){1-exp(-eta1*t)}
for (i in 1:length(DCO)){e[i] = fe(DCO = DCO[i], r = 1, h0 = h0, S0 = S0, h1 = h1, S1 = S1, 
Lambda = F.entry, n = 100, G0=G0, G1=G1)$e}

#(a) Study design using weighted logrank test option 1
wlr.power.maxcombo(DCO = c(24, 36), overall.alpha=0.025, sf = "LDOF", 
  r = 1, n = 100, h0 = h0, S0=S0, h1 = h1, S1= S1, 
  f.ws = fws1, Lambda=F.entry, G0=G0, G1=G1, 
  mu.method = "Schoenfeld", cov.method = "H0")
  
#(b) Study design using weighting strategy 2
wlr.power.maxcombo(DCO = c(24, 36), overall.alpha=0.025, sf = "LDOF", 
  r = 1, n = 100, h0 = h0, S0=S0, h1 = h1, S1= S1, 
  f.ws = fws2, Lambda=F.entry, G0=G0, G1=G1, 
  mu.method = "Schoenfeld", cov.method = "H0")
  
#(c) Simulations for exploring weighted logrank tests option 1 and 2

#(c1) Type I error; also output logrank test simulation results
H1 = simulation.nphDesign.pwexp(nSim=5, N = 100, r = 1, 
A = 21, w=1.5,
lam0=lambda0, lam1=lambda0*0.7, cuts0=NULL, cuts1=NULL,
targetEvents = e, drop0 = 0.03/12, drop1=0.03/12,
overall.alpha = 0.025, sf = "LDOF",
H0 = "N", logrank="Y", fws.options=list(fws1))

#same as above; using F.entry function to replac A and w specifications.
H0 = simulation.nphDesign.pwexp(nSim=5, N = 100, r=1, 
Lambda=F.entry, 
lam0=lambda0, lam1=lambda0*0.7, cuts0=NULL, cuts1=NULL,
targetEvents = e, drop0 = 0.03/12, drop1=0.03/12,
overall.alpha = 0.025, sf = "LDOF",
H0 = "Y", logrank="Y", fws.options=list(fws1))

#same as above using the general function
o=simulation.nphDesign(nSim=5, n = 100, r=1, Lambda=F.entry, drop0=0.03/12, drop1=0.03/12, 
dist0 = "exponential", lam0=lambda0, shape0 = NULL, scale0 = NULL, p10 = NULL, S0 = NULL, cuts0 = NULL,
dist1 = "exponential", lam1=lambda0, shape1 = NULL, scale1 = NULL, p11 = NULL, S1 = NULL, cuts1 = NULL, 
targetEvents = e, sf = "LDOF", param = NULL, overall.alpha = 0.025, p1=NULL, cum.alpha=NULL,
logrank="Y", fws.options=list(fws1), 
parallel=FALSE, n.cores=8, seed=2022, out.z = TRUE)

#same as above using "Proportional Hazards" option
o=simulation.nphDesign(nSim=5, n = 100, r=1, Lambda=F.entry, drop0=0.03/12, drop1=0.03/12, 
dist0 = "exponential", lam0=lambda0, shape0 = NULL, scale0 = NULL, p10 = NULL, S0 = NULL, cuts0 = NULL,
dist1 = "Proportional Hazards", HR=0.7, lam1=NULL, shape1 = NULL, scale1 = NULL, p11 = NULL, S1 = NULL, cuts1 = NULL, 
targetEvents = e, sf = "LDOF", param = NULL, overall.alpha = 0.025, p1=NULL, cum.alpha=NULL,
logrank="Y", fws.options=list(fws1), 
parallel=FALSE, n.cores=8, seed=2022, out.z = FALSE)

#same as above using "Proportional Hazards" option
o=simulation.nphDesign(nSim=100, n = 100, r=1, Lambda=F.entry, 
drop0=0.03/12, drop1=0.03/12, 
dist0 = "exponential", lam0=lambda0, shape0 = NULL, scale0 = NULL, 
p10 = NULL, S0 = NULL, cuts0 = NULL,
dist1 = "Proportional Hazards", HR=0.7, lam1=NULL, shape1 = NULL, 
scale1 = NULL, p11 = NULL, S1 = NULL, cuts1 = NULL, 
targetEvents = e, sf = "LDOF", param = NULL, overall.alpha = 0.025, 
p1=NULL, cum.alpha=NULL,
logrank="Y", fws.options=list(fws1), 
parallel=TRUE, n.cores=10, seed=2022, out.z = FALSE)

}
